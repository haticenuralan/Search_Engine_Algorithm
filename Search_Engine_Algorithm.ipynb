{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9XytaYU1RIeyL81L/7lrP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haticenuralan/Search_Engine_Algorithm/blob/main/Search_Engine_Algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definitions of procedures\n",
        "\n",
        "**1.get_page(url):** This function takes a URL as input and tries to fetch the\n",
        "content of the webpage at that URL. It returns the page content as a string. If it encounters an error (like the page doesn't exist), it returns an empty string.\n",
        "\n",
        "**2.get_next_target(page):** This function finds the next hyperlink in the given page content. It returns the URL of the hyperlink and the position where the hyperlink ends in the page content. If no hyperlink is found, it returns None and 0.\n",
        "\n",
        "**3. get_all_links(page):** It extracts all hyperlinks from the given page content. This is done by repeatedly calling get_next_target and collecting the URLs until no more links are found. It returns a list of URLs.\n",
        "\n",
        "**3.union(p,q):** This function takes two lists p and q and adds elements from q to p if they are not already in p. It's used to avoid duplicate URLs in the list of pages to crawl.\n",
        "\n",
        "**4. add_to_index(index, keyword, url):** This function adds a keyword and its corresponding URL to an index, which is a dictionary. If the keyword already exists in the index, the URL is added to the list of URLs associated with that keyword. If not, a new entry is created.\n",
        "\n",
        "**5.getClearPage(content):** This function extracts the title and body from a webpage's HTML content, removing all HTML tags from the body. It returns the concatenated title and body as a clean text.\n",
        "\n",
        "**6.addPageToIndex(index, url, content):** This function adds all the words found in the content of a page to the index with their corresponding URL. It uses getClearPage to clean the HTML content and add_to_index to add words to the index.\n",
        "\n",
        "**7.lookup(index, keyword):** This function searches for a keyword in the index and returns a list of URLs associated with that keyword. If the keyword is not found, it returns None.\n",
        "\n",
        "**8. computeRanks(graph):** This function calculates a rank for each page in a web graph, based on a simplified version of Google's PageRank algorithm. Pages are ranked higher if they are linked to by many pages or by pages that are themselves highly ranked.\n",
        "\n",
        "**9.crawlWeb(seed):** This is the main function of the web crawler. It starts with a seed page and crawls the web from there, using get_page to fetch content, addPageToIndex to index it, get_all_links to find new pages to crawl, and union to add them to the list of pages to crawl. It keeps track of pages it has already crawled to avoid revisiting them. The function returns the index (word to URL mapping) and a graph of outlinks (which page links to which).\n",
        "\n",
        "Finally, the script starts by crawling the web starting from a seed page (seedpage) and then computes the ranks of the pages in the crawled web graph."
      ],
      "metadata": {
        "id": "vSGBJafjjLLI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lwj6IPBaVBMT"
      },
      "outputs": [],
      "source": [
        "def get_page(url):\n",
        "  try:\n",
        "    import urllib.request\n",
        "    page = urllib.request.urlopen(url).read()\n",
        "    page = page.decode(\"utf-8\")\n",
        "    return page\n",
        "  except:\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "def get_next_target(page):\n",
        "    start_link = page.find('<a href=')\n",
        "    if start_link == -1:\n",
        "        return None, 0\n",
        "    start_quote = page.find('\"', start_link)\n",
        "    end_quote = page.find('\"', start_quote+1)\n",
        "    url = page[start_quote + 1:end_quote]\n",
        "    return url, end_quote\n",
        "\n",
        "\n",
        "def get_all_links(page):\n",
        "    links = []\n",
        "    while True:\n",
        "      url, endpos = get_next_target(page)\n",
        "      if url:\n",
        "        links.append(url)\n",
        "        page = page[endpos:]\n",
        "      else:\n",
        "        break\n",
        "    return links\n",
        "\n",
        "\n",
        "def union(p,q):\n",
        "    for e in q:\n",
        "        if e not in p:\n",
        "            p.append(e)\n",
        "\n",
        "\n",
        "def add_to_index(index, keyword, url):\n",
        "  if keyword in index:\n",
        "    index[keyword].append(url)\n",
        "  else:\n",
        "    index[keyword] = [url]\n",
        "\n",
        "\n",
        "def getClearPage(content):\n",
        "  title = content[content.find(\"<title>\")+7:content.find(\"</title>\")]\n",
        "  body = content[content.find(\"<body>\")+6:content.find(\"</body>\")]\n",
        "  while body.find(\">\") != -1:\n",
        "    start =  body.find(\"<\")\n",
        "    end =  body.find(\">\")\n",
        "    body = body[:start] + body[end+1:]\n",
        "  return title + body\n",
        "\n",
        "\n",
        "def addPageToIndex(index, url, content):\n",
        "  content = getClearPage(content)\n",
        "  words = content.split()\n",
        "  for word in words:\n",
        "    add_to_index(index, word, url)\n",
        "\n",
        "\n",
        "def lookup(index, keyword):\n",
        "  if keyword in index:\n",
        "    return index[keyword]\n",
        "  else:\n",
        "    return None\n",
        "\n",
        "def computeRanks(graph):\n",
        "  damping = 0.8\n",
        "  numloops = 10\n",
        "  ranks = {}\n",
        "  npages = len(graph)\n",
        "  for page in graph:\n",
        "    ranks[page] = 1/npages\n",
        "  for i in range(0, numloops):\n",
        "    newranks = {}\n",
        "    for page in graph:\n",
        "      newrank = (1-damping)/npages\n",
        "      for node in graph:\n",
        "        if page in graph[node]:\n",
        "          newrank = newrank + damping*(ranks[node]/ len(graph[node]) )\n",
        "      newranks[page] = newrank\n",
        "    ranks = newranks\n",
        "  return ranks\n",
        "\n",
        "\n",
        "def crawlWeb(seed):\n",
        "    tocrawl = [seed]\n",
        "    crawled = []\n",
        "    graph = {}\n",
        "    index = {}\n",
        "    while tocrawl:\n",
        "        page = tocrawl.pop()\n",
        "        if page not in crawled:\n",
        "            content = get_page(page)\n",
        "            addPageToIndex(index, page, content)\n",
        "            outlinks = get_all_links(content)\n",
        "            graph[page] = outlinks\n",
        "            union(tocrawl, outlinks)\n",
        "            crawled.append(page)\n",
        "    return index, graph  # returns index, graph of outlinks now\n",
        "\n",
        "\n",
        "seedpage = \"https://ayaktayolcukalmasin.com.tr/ana_sayfa.html\"\n",
        "index, graph = crawlWeb(seedpage)\n",
        "ranks = computeRanks(graph)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The graph has {len(graph)} elements. These are:\")\n",
        "n = 1\n",
        "for key,value in graph.items():\n",
        "  print(\"\\t\"+str(n)+\". \", key, \":\", value)\n",
        "  n = n+1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPeAUQ98WBsi",
        "outputId": "bee358ee-d13d-4e8c-b014-5194e6be25cd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The graph has 6 elements. These are:\n",
            "\t1.  https://ayaktayolcukalmasin.com.tr/ana_sayfa.html : ['https://ayaktayolcukalmasin.com.tr/ankara.html', 'https://ayaktayolcukalmasin.com.tr/konya.html', 'https://ayaktayolcukalmasin.com.tr/istanbul.html', 'https://ayaktayolcukalmasin.com.tr/oktayrecommends.html', 'https://ayaktayolcukalmasin.com.tr/seymarecommends.html']\n",
            "\t2.  https://ayaktayolcukalmasin.com.tr/seymarecommends.html : ['https://ayaktayolcukalmasin.com.tr/oktayrecommends.html', 'https://ayaktayolcukalmasin.com.tr/konya.html']\n",
            "\t3.  https://ayaktayolcukalmasin.com.tr/oktayrecommends.html : ['https://ayaktayolcukalmasin.com.tr/istanbul.html']\n",
            "\t4.  https://ayaktayolcukalmasin.com.tr/istanbul.html : []\n",
            "\t5.  https://ayaktayolcukalmasin.com.tr/konya.html : ['https://ayaktayolcukalmasin.com.tr/seymarecommends.html']\n",
            "\t6.  https://ayaktayolcukalmasin.com.tr/ankara.html : []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for key, value in ranks.items():\n",
        "  print(\"The rank of the page\", key,\":\\t\", value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7rfCXSAYeLv",
        "outputId": "548f356e-aff7-4294-e84f-9f25181e20cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The rank of the page https://ayaktayolcukalmasin.com.tr/ana_sayfa.html :\t 0.033333333333333326\n",
            "The rank of the page https://ayaktayolcukalmasin.com.tr/seymarecommends.html :\t 0.10274769919999999\n",
            "The rank of the page https://ayaktayolcukalmasin.com.tr/oktayrecommends.html :\t 0.07998944255999998\n",
            "The rank of the page https://ayaktayolcukalmasin.com.tr/istanbul.html :\t 0.10274769919999999\n",
            "The rank of the page https://ayaktayolcukalmasin.com.tr/konya.html :\t 0.07998944255999998\n",
            "The rank of the page https://ayaktayolcukalmasin.com.tr/ankara.html :\t 0.038666666666666655\n"
          ]
        }
      ]
    }
  ]
}